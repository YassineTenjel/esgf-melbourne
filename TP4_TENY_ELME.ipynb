{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "import numpy as np\n",
    "import urllib.request\n",
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "from io import StringIO\n",
    "from matplotlib import pyplot as plt\n",
    "from datetime import timedelta\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, SimpleRNN\n",
    "from keras.layers import LSTM,Dense ,Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_melbourne_data() -> pd.DataFrame:\n",
    "    '''\n",
    "    Returns a dataframe of the melbourne data set.\n",
    "    :return: pd.DataFrame\n",
    "    '''\n",
    "\n",
    "    # URL of the raw csv data to download\n",
    "    raw_url = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/daily-min-temperatures.csv\"\n",
    "\n",
    "    # Get the earthquake data from the API\n",
    "    response = urllib.request.urlopen(raw_url)\n",
    "\n",
    "    # Decode earthquake data\n",
    "    response = response.read().decode('utf-8')\n",
    "\n",
    "    # Return as a pandas dataframe\n",
    "    data = pd.read_csv(StringIO(response))\n",
    "\n",
    "    # Cast the date column to datetime\n",
    "    data['Date'] = pd.to_datetime(data['Date'])\n",
    "\n",
    "    return data\n",
    "\n",
    "time_serie =get_melbourne_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_test_data(melbourne_data: pd.DataFrame, split_year: str=\"1987\") -> (pd.DataFrame, pd.DataFrame):\n",
    "    '''\n",
    "    Split the melbourne data into a training dataframe and a test dataframe.\n",
    "    The training data is composed of all temperature points strictly anterior to the given split year.\n",
    "    The test data is composed of all the points posterior or equal to the split year.\n",
    "    :param melbourne_data: pd.DataFrame, with at least column ['Date']\n",
    "    :param split_year: str, the year to split the data on\n",
    "    :return: (pd.DataFrame, pd.DataFrame)\n",
    "    '''\n",
    "\n",
    "    # Format split year variable\n",
    "    split_year = \"{}\".format(int(split_year) - 1)\n",
    "\n",
    "    # Trainings data. Data anterior to the given split year\n",
    "    train_data = melbourne_data.loc[:split_year]\n",
    "\n",
    "    # Test data. Data posterior or equal to the given split year\n",
    "    test_data = melbourne_data.loc[split_year:]\n",
    "\n",
    "    return train_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_training_point(data, t_str, history_days=64, horizon_days=1):\n",
    "    '''\n",
    "    :param data:\n",
    "    :param t_str:\n",
    "    :param history_days:\n",
    "    :param horizon_days:\n",
    "    :return:\n",
    "    '''\n",
    "\n",
    "    # Cast for indexing\n",
    "    t_datetime = datetime.strptime(t_str, \"%Y-%m-%d 00:00:00\")\n",
    "\n",
    "    # Create training example (x,y)\n",
    "    try:\n",
    "        x = data.loc[t_datetime - timedelta(days=history_days - 1):t_datetime]\n",
    "        y = data.loc[t_datetime + timedelta(days=1):t_datetime + timedelta(days=horizon_days)]\n",
    "    except KeyError:\n",
    "        raise KeyError(\"The date {} is not in the data\".format(t_str))\n",
    "\n",
    "    # Return\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_training_points(data, history_days, horizon_days):\n",
    "    '''\n",
    "    :param data:\n",
    "    :param history_days:\n",
    "    :param horizon_days:\n",
    "    :return:\n",
    "    '''\n",
    "    X = []\n",
    "    Y = []\n",
    "    for t in data.index[history_days:(len(data) - horizon_days)]:\n",
    "        try:\n",
    "            x, y = build_training_point(data, str(t), history_days=history_days, horizon_days=horizon_days)\n",
    "            if (len(x) == history_days) & (len(y) == horizon_days):\n",
    "                X.append(x)\n",
    "                Y.append(y)\n",
    "        except KeyError:\n",
    "            continue\n",
    "    X = np.stack(X)\n",
    "    Y = np.stack(Y)\n",
    "    return X, Y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(year):\n",
    "    split_date = pd.datetime(year,1,1)\n",
    "    train_data = time_serie.loc[time_serie['Date'] < split_date]\n",
    "    test_data = time_serie.loc[time_serie['Date'] >= split_date]\n",
    "\n",
    "    scaler = MinMaxScaler()\n",
    "    train_data[[\"Temp\"]] = scaler.fit_transform(train_data[[\"Temp\"]] )\n",
    "    test_data[[\"Temp\"]] = scaler.fit_transform(test_data[[\"Temp\"]] )\n",
    "\n",
    "    # Index\n",
    "    train_data.set_index('Date', inplace=True)\n",
    "    test_data.set_index('Date', inplace=True)\n",
    "    return train_data, test_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Bidirectional \n",
    "from keras.layers import Input, LSTM, Dense\n",
    "from keras.models import Model\n",
    "def model_LSTM(history_days,horizon_days):\n",
    "    model_input = Input(shape=(history_days, 1))\n",
    "    model = Sequential()\n",
    "    z=model\n",
    "    z = Bidirectional(LSTM(units=30, return_sequences=True))(model_input)\n",
    "    z = Dropout(0.2)(z)\n",
    "    z= LSTM(units= 30 , return_sequences=True)(z)\n",
    "    z= Dropout(0.2)(z)\n",
    "    z = LSTM(units= 30 , return_sequences=True)(z)\n",
    "    z=Dropout(0.2)(z)\n",
    "    z=LSTM(units= 30)(z)\n",
    "    z=Dropout(0.2)(z)\n",
    "    z= Dense(horizon_days,activation='linear')(z)\n",
    "    Model_Lstm= Model(inputs=model_input, outputs=z)\n",
    "    return Model_Lstm\n",
    "model_3m = model_LSTM(90, 30)\n",
    "model_6m = model_LSTM(180, 30)\n",
    "model_12m = model_LSTM(365, 30)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/32\n",
      "61/61 [==============================] - 12s 97ms/step - loss: 0.0702\n",
      "Epoch 2/32\n",
      "61/61 [==============================] - 6s 96ms/step - loss: 0.0255\n",
      "Epoch 3/32\n",
      "61/61 [==============================] - 6s 93ms/step - loss: 0.0219\n",
      "Epoch 4/32\n",
      "61/61 [==============================] - 6s 98ms/step - loss: 0.0199\n",
      "Epoch 5/32\n",
      "61/61 [==============================] - 6s 96ms/step - loss: 0.0183\n",
      "Epoch 6/32\n",
      "61/61 [==============================] - 6s 96ms/step - loss: 0.0173\n",
      "Epoch 7/32\n",
      "61/61 [==============================] - 6s 94ms/step - loss: 0.0168\n",
      "Epoch 8/32\n",
      "61/61 [==============================] - 6s 94ms/step - loss: 0.0158\n",
      "Epoch 9/32\n",
      "61/61 [==============================] - 6s 94ms/step - loss: 0.0152\n",
      "Epoch 10/32\n",
      "61/61 [==============================] - 6s 96ms/step - loss: 0.0152\n",
      "Epoch 11/32\n",
      "61/61 [==============================] - 6s 97ms/step - loss: 0.0149\n",
      "Epoch 12/32\n",
      "61/61 [==============================] - 6s 94ms/step - loss: 0.0148\n",
      "Epoch 13/32\n",
      "61/61 [==============================] - 6s 94ms/step - loss: 0.0145\n",
      "Epoch 14/32\n",
      "61/61 [==============================] - 6s 94ms/step - loss: 0.0141\n",
      "Epoch 15/32\n",
      "61/61 [==============================] - 6s 94ms/step - loss: 0.0136\n",
      "Epoch 16/32\n",
      "61/61 [==============================] - 6s 98ms/step - loss: 0.0136\n",
      "Epoch 17/32\n",
      "61/61 [==============================] - 6s 99ms/step - loss: 0.0133\n",
      "Epoch 18/32\n",
      "61/61 [==============================] - 6s 101ms/step - loss: 0.0134\n",
      "Epoch 19/32\n",
      "61/61 [==============================] - 6s 96ms/step - loss: 0.0130\n",
      "Epoch 20/32\n",
      "61/61 [==============================] - 6s 94ms/step - loss: 0.0126\n",
      "Epoch 21/32\n",
      "61/61 [==============================] - 6s 97ms/step - loss: 0.0127\n",
      "Epoch 22/32\n",
      "61/61 [==============================] - 6s 92ms/step - loss: 0.0127\n",
      "Epoch 23/32\n",
      "61/61 [==============================] - 6s 96ms/step - loss: 0.0125\n",
      "Epoch 24/32\n",
      "61/61 [==============================] - 6s 95ms/step - loss: 0.0126\n",
      "Epoch 25/32\n",
      "61/61 [==============================] - 6s 95ms/step - loss: 0.0126\n",
      "Epoch 26/32\n",
      "61/61 [==============================] - 6s 103ms/step - loss: 0.0125\n",
      "Epoch 27/32\n",
      "61/61 [==============================] - 6s 97ms/step - loss: 0.0124\n",
      "Epoch 28/32\n",
      "61/61 [==============================] - 6s 101ms/step - loss: 0.0122\n",
      "Epoch 29/32\n",
      "61/61 [==============================] - 6s 103ms/step - loss: 0.0123\n",
      "Epoch 30/32\n",
      "61/61 [==============================] - 6s 96ms/step - loss: 0.0121\n",
      "Epoch 31/32\n",
      "61/61 [==============================] - 6s 96ms/step - loss: 0.0121\n",
      "Epoch 32/32\n",
      "61/61 [==============================] - 6s 96ms/step - loss: 0.0121\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x147904aab20>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data, test_data = split_data(1987)\n",
    "X_train, Y_train = create_training_points(train_data, history_days=90, horizon_days=30)\n",
    "model_3m.compile(optimizer='adam', loss='mse')\n",
    "model_3m.fit(X_train, Y_train, epochs=32, batch_size=32, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1987, k=3:  0.02679071054359822\n"
     ]
    }
   ],
   "source": [
    "train_data, test_data = split_data(1987)\n",
    "X_test, Y_test = create_training_points(test_data, history_days=90, horizon_days=30)\n",
    "y_pred_3m =model_3m.predict(X_test)\n",
    "print(\"1987, k=3: \", mean_squared_error(y_pred_3m[0], Y_test[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/32\n",
      "56/56 [==============================] - 16s 185ms/step - loss: 0.0712\n",
      "Epoch 2/32\n",
      "56/56 [==============================] - 10s 187ms/step - loss: 0.0256\n",
      "Epoch 3/32\n",
      "56/56 [==============================] - 11s 189ms/step - loss: 0.0223\n",
      "Epoch 4/32\n",
      "56/56 [==============================] - 10s 187ms/step - loss: 0.0210\n",
      "Epoch 5/32\n",
      "56/56 [==============================] - 11s 187ms/step - loss: 0.0189\n",
      "Epoch 6/32\n",
      "56/56 [==============================] - 10s 185ms/step - loss: 0.0178\n",
      "Epoch 7/32\n",
      "56/56 [==============================] - 11s 189ms/step - loss: 0.0166\n",
      "Epoch 8/32\n",
      "56/56 [==============================] - 11s 190ms/step - loss: 0.0156\n",
      "Epoch 9/32\n",
      "56/56 [==============================] - 10s 186ms/step - loss: 0.0151\n",
      "Epoch 10/32\n",
      "56/56 [==============================] - 10s 183ms/step - loss: 0.0145\n",
      "Epoch 11/32\n",
      "56/56 [==============================] - 10s 184ms/step - loss: 0.0145\n",
      "Epoch 12/32\n",
      "56/56 [==============================] - 11s 188ms/step - loss: 0.0141\n",
      "Epoch 13/32\n",
      "56/56 [==============================] - 10s 181ms/step - loss: 0.0139\n",
      "Epoch 14/32\n",
      "56/56 [==============================] - 10s 181ms/step - loss: 0.0136\n",
      "Epoch 15/32\n",
      "56/56 [==============================] - 10s 184ms/step - loss: 0.0134\n",
      "Epoch 16/32\n",
      "56/56 [==============================] - 10s 182ms/step - loss: 0.0132\n",
      "Epoch 17/32\n",
      "56/56 [==============================] - 11s 192ms/step - loss: 0.0130\n",
      "Epoch 18/32\n",
      "56/56 [==============================] - 11s 191ms/step - loss: 0.0135\n",
      "Epoch 19/32\n",
      "56/56 [==============================] - 11s 190ms/step - loss: 0.0128\n",
      "Epoch 20/32\n",
      "56/56 [==============================] - 11s 189ms/step - loss: 0.0126\n",
      "Epoch 21/32\n",
      "56/56 [==============================] - 10s 185ms/step - loss: 0.0125\n",
      "Epoch 22/32\n",
      "56/56 [==============================] - 10s 183ms/step - loss: 0.0124\n",
      "Epoch 23/32\n",
      "56/56 [==============================] - 10s 182ms/step - loss: 0.0124\n",
      "Epoch 24/32\n",
      "56/56 [==============================] - 10s 186ms/step - loss: 0.0122\n",
      "Epoch 25/32\n",
      "56/56 [==============================] - 11s 193ms/step - loss: 0.0121\n",
      "Epoch 26/32\n",
      "56/56 [==============================] - 11s 189ms/step - loss: 0.0120\n",
      "Epoch 27/32\n",
      "56/56 [==============================] - 10s 184ms/step - loss: 0.0122\n",
      "Epoch 28/32\n",
      "56/56 [==============================] - 10s 182ms/step - loss: 0.0123\n",
      "Epoch 29/32\n",
      "56/56 [==============================] - 10s 187ms/step - loss: 0.0118\n",
      "Epoch 30/32\n",
      "56/56 [==============================] - 10s 183ms/step - loss: 0.0118\n",
      "Epoch 31/32\n",
      "56/56 [==============================] - 10s 186ms/step - loss: 0.0118\n",
      "Epoch 32/32\n",
      "56/56 [==============================] - 10s 187ms/step - loss: 0.0119\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1479d84ba00>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data, test_data = split_data(1987)\n",
    "X_train, Y_train = create_training_points(train_data, history_days=180, horizon_days=30)\n",
    "model_6m.compile(optimizer='adam', loss='mse')\n",
    "model_6m.fit(X_train, Y_train, epochs=32, batch_size=32, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1987, k=6:  0.008257268078829852\n"
     ]
    }
   ],
   "source": [
    "train_data, test_data = split_data(1987)\n",
    "X_test, Y_test = create_training_points(test_data, history_days=180, horizon_days=30)\n",
    "y_pred_6m = model_6m.predict(X_test)\n",
    "print(\"1987, k=6: \", mean_squared_error(y_pred_6m[0], Y_test[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/32\n",
      "44/44 [==============================] - 23s 398ms/step - loss: 0.0787\n",
      "Epoch 2/32\n",
      "44/44 [==============================] - 17s 376ms/step - loss: 0.0298\n",
      "Epoch 3/32\n",
      "44/44 [==============================] - 17s 380ms/step - loss: 0.0248\n",
      "Epoch 4/32\n",
      "44/44 [==============================] - 17s 380ms/step - loss: 0.0231\n",
      "Epoch 5/32\n",
      "44/44 [==============================] - 18s 402ms/step - loss: 0.0209\n",
      "Epoch 6/32\n",
      "44/44 [==============================] - 18s 398ms/step - loss: 0.0204\n",
      "Epoch 7/32\n",
      "44/44 [==============================] - 17s 387ms/step - loss: 0.0192\n",
      "Epoch 8/32\n",
      "44/44 [==============================] - 18s 397ms/step - loss: 0.0189\n",
      "Epoch 9/32\n",
      "44/44 [==============================] - 17s 381ms/step - loss: 0.0179\n",
      "Epoch 10/32\n",
      "44/44 [==============================] - 16s 372ms/step - loss: 0.0180\n",
      "Epoch 11/32\n",
      "44/44 [==============================] - 17s 380ms/step - loss: 0.0176\n",
      "Epoch 12/32\n",
      "44/44 [==============================] - 17s 391ms/step - loss: 0.0171\n",
      "Epoch 13/32\n",
      "44/44 [==============================] - 18s 398ms/step - loss: 0.0162\n",
      "Epoch 14/32\n",
      "44/44 [==============================] - 17s 380ms/step - loss: 0.0154\n",
      "Epoch 15/32\n",
      "44/44 [==============================] - 17s 387ms/step - loss: 0.0149\n",
      "Epoch 16/32\n",
      "44/44 [==============================] - 17s 386ms/step - loss: 0.0150\n",
      "Epoch 17/32\n",
      "44/44 [==============================] - 18s 400ms/step - loss: 0.0145\n",
      "Epoch 18/32\n",
      "44/44 [==============================] - 17s 396ms/step - loss: 0.0139\n",
      "Epoch 19/32\n",
      "44/44 [==============================] - 18s 404ms/step - loss: 0.0138\n",
      "Epoch 20/32\n",
      "44/44 [==============================] - 18s 400ms/step - loss: 0.0136\n",
      "Epoch 21/32\n",
      "44/44 [==============================] - 17s 391ms/step - loss: 0.0135\n",
      "Epoch 22/32\n",
      "44/44 [==============================] - 17s 389ms/step - loss: 0.0134\n",
      "Epoch 23/32\n",
      "44/44 [==============================] - 18s 399ms/step - loss: 0.0133\n",
      "Epoch 24/32\n",
      "44/44 [==============================] - 17s 393ms/step - loss: 0.0133\n",
      "Epoch 25/32\n",
      "44/44 [==============================] - 17s 389ms/step - loss: 0.0132\n",
      "Epoch 26/32\n",
      "44/44 [==============================] - 17s 388ms/step - loss: 0.0129\n",
      "Epoch 27/32\n",
      "44/44 [==============================] - 17s 387ms/step - loss: 0.0130\n",
      "Epoch 28/32\n",
      "44/44 [==============================] - 17s 391ms/step - loss: 0.0130\n",
      "Epoch 29/32\n",
      "44/44 [==============================] - 17s 392ms/step - loss: 0.0128\n",
      "Epoch 30/32\n",
      "44/44 [==============================] - 17s 397ms/step - loss: 0.0128\n",
      "Epoch 31/32\n",
      "44/44 [==============================] - 17s 386ms/step - loss: 0.0126\n",
      "Epoch 32/32\n",
      "44/44 [==============================] - 17s 379ms/step - loss: 0.0127\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x147aac45280>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data, test_data = split_data(1987)\n",
    "X_train, Y_train = create_training_points(train_data, history_days=365, horizon_days=30)\n",
    "model_12m.compile(optimizer='adam', loss='mse')\n",
    "model_12m.fit(X_train, Y_train, epochs=32, batch_size=32, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1987, k=12:  0.027797010512636673\n"
     ]
    }
   ],
   "source": [
    "train_data, test_data = split_data(1987)\n",
    "X_test, Y_test = create_training_points(test_data, history_days=365, horizon_days=30)\n",
    "y_pred_12m = model_12m.predict(X_test)\n",
    "print(\"1987, k=12: \", mean_squared_error(y_pred_12m[0], Y_test[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1988, k=3:  0.04021030996877787\n"
     ]
    }
   ],
   "source": [
    "train_data, test_data = split_data(1988)\n",
    "X_test, Y_test = create_training_points(test_data, history_days=90, horizon_days=30)\n",
    "y_pred_3m = model_3m.predict(X_test)\n",
    "print(\"1988, k=3: \", mean_squared_error(y_pred_3m[0], Y_train[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1988, k=6:  0.15121333047448135\n"
     ]
    }
   ],
   "source": [
    "train_data, test_data = split_data(1988)\n",
    "X_test, Y_test = create_training_points(test_data, history_days=180, horizon_days=30)\n",
    "y_pred_6m = model_6m.predict(X_test)\n",
    "print(\"1988, k=6: \", mean_squared_error(y_pred_6m[0], Y_train[1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1988, k=12:  0.018993083323516693\n"
     ]
    }
   ],
   "source": [
    "train_data, test_data = split_data(1988)\n",
    "X_test, Y_test = create_training_points(test_data, history_days=365, horizon_days=30)\n",
    "y_pred_12m = model_12m.predict(X_test)\n",
    "print(\"1988, k=12: \", mean_squared_error(y_pred_12m[0], Y_train[1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1989, k=3:  0.04610006463221635\n"
     ]
    }
   ],
   "source": [
    "train_data, test_data = split_data(1989)\n",
    "X_test, Y_test = create_training_points(test_data, history_days=90, horizon_days=30)\n",
    "y_pred_3m = model_3m.predict(X_test)\n",
    "print(\"1989, k=3: \", mean_squared_error(y_pred_3m[0], Y_train[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1989, k=6:  0.16455930190496026\n"
     ]
    }
   ],
   "source": [
    "train_data, test_data = split_data(1989)\n",
    "X_test, Y_test = create_training_points(test_data, history_days=180, horizon_days=30)\n",
    "y_pred_6m = model_6m.predict(X_test)\n",
    "print(\"1989, k=6: \", mean_squared_error(y_pred_6m[0], Y_train[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1989, k=12:  0.01989014650220999\n"
     ]
    }
   ],
   "source": [
    "train_data, test_data = split_data(1989)\n",
    "X_test, Y_test = create_training_points(test_data, history_days=365, horizon_days=30)\n",
    "y_pred_12m = model_12m.predict(X_test)\n",
    "print(\"1989, k=12: \", mean_squared_error(y_pred_12m[0], Y_train[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|   *Evaluation year* \t| *Next 3 months* \t| *Next 6 months* \t| *Next 12 months* \t|\n",
    "|:---------------------:\t|:-----------------:\t|:-----------------:\t|:------------------:\t|\n",
    "|        *1987*       \t|       mse =  0.0267\t|    mse =  0.008257   \t|     mse =  0.0277   \t|\n",
    "|        *1988*       \t|       mse =  0.0402\t|    mse =  0.1512    \t|     mse =  0.0189\t|\n",
    "|        *1989*       \t|       mse =  0.0461\t|    mse =  0.1645   \t|     mse =  0.0198  \t|"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
